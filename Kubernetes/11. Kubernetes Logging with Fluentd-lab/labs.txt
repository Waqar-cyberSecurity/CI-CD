Lab 11: Kubernetes Logging with Fluentd
Lab Objectives
By the end of this lab, you will be able to:

Install and configure Fluentd as a DaemonSet in Kubernetes
Set up centralized logging collection from Kubernetes pods
Configure Fluentd to forward logs to Elasticsearch
Deploy Elasticsearch and Kibana for log visualization
Test and verify the complete logging pipeline
Understand the architecture of centralized logging in Kubernetes
Prerequisites
Before starting this lab, you should have:

Basic understanding of Kubernetes concepts (pods, services, deployments)
Familiarity with YAML configuration files
Basic knowledge of Linux command line operations
Understanding of containerization concepts
Basic knowledge of logging concepts
Lab Environment
Al Nafi provides Linux-based cloud machines for this lab. Simply click Start Lab to access your dedicated Linux machine. The provided machine is bare metal with no pre-installed tools, so you will install all required components during the lab exercises.

Task 1: Install and Configure Kubernetes Environment
Subtask 1.1: Install Docker
First, we need to install Docker as our container runtime.

# Update system packages
sudo apt update

# Install required packages
sudo apt install -y apt-transport-https ca-certificates curl gnupg lsb-release

# Add Docker's official GPG key
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg

# Add Docker repository
echo "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# Update package index
sudo apt update

# Install Docker
sudo apt install -y docker-ce docker-ce-cli containerd.io

# Add current user to docker group
sudo usermod -aG docker $USER

# Start and enable Docker
sudo systemctl start docker
sudo systemctl enable docker
Subtask 1.2: Install Kubernetes Components
Install kubectl, kubeadm, and kubelet:

# Add Kubernetes GPG key
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

# Add Kubernetes repository
echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

# Update package index
sudo apt update

# Install Kubernetes components
sudo apt install -y kubelet kubeadm kubectl

# Hold packages to prevent automatic updates
sudo apt-mark hold kubelet kubeadm kubectl
Subtask 1.3: Initialize Kubernetes Cluster
# Initialize the cluster
sudo kubeadm init --pod-network-cidr=10.244.0.0/16

# Set up kubectl for regular user
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Install Flannel network plugin
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

# Remove taint from master node to allow pod scheduling
kubectl taint nodes --all node-role.kubernetes.io/control-plane-
Subtask 1.4: Verify Kubernetes Installation
# Check cluster status
kubectl get nodes

# Check system pods
kubectl get pods -n kube-system

# Wait for all pods to be running
kubectl wait --for=condition=Ready pods --all -n kube-system --timeout=300s
Task 2: Deploy Elasticsearch and Kibana
Subtask 2.1: Create Logging Namespace
# Create namespace for logging components
kubectl create namespace logging
Subtask 2.2: Deploy Elasticsearch
Create the Elasticsearch deployment configuration:

cat > elasticsearch.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
spec:
  replicas: 1
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
        ports:
        - containerPort: 9200
        - containerPort: 9300
        env:
        - name: discovery.type
          value: single-node
        - name: ES_JAVA_OPTS
          value: "-Xms512m -Xmx512m"
        - name: xpack.security.enabled
          value: "false"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        volumeMounts:
        - name: elasticsearch-data
          mountPath: /usr/share/elasticsearch/data
      volumes:
      - name: elasticsearch-data
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
spec:
  selector:
    app: elasticsearch
  ports:
  - port: 9200
    targetPort: 9200
    name: http
  - port: 9300
    targetPort: 9300
    name: transport
EOF

# Deploy Elasticsearch
kubectl apply -f elasticsearch.yaml
Subtask 2.3: Deploy Kibana
Create the Kibana deployment configuration:

cat > kibana.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: logging
  labels:
    app: kibana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      containers:
      - name: kibana
        image: docker.elastic.co/kibana/kibana:7.17.0
        ports:
        - containerPort: 5601
        env:
        - name: ELASTICSEARCH_HOSTS
          value: "http://elasticsearch:9200"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: logging
  labels:
    app: kibana
spec:
  selector:
    app: kibana
  ports:
  - port: 5601
    targetPort: 5601
    nodePort: 30601
  type: NodePort
EOF

# Deploy Kibana
kubectl apply -f kibana.yaml
Subtask 2.4: Verify Elasticsearch and Kibana Deployment
# Check if pods are running
kubectl get pods -n logging

# Wait for pods to be ready
kubectl wait --for=condition=Ready pods --all -n logging --timeout=300s

# Check services
kubectl get services -n logging

# Test Elasticsearch connectivity
kubectl port-forward -n logging svc/elasticsearch 9200:9200 &
sleep 5
curl -X GET "localhost:9200/_cluster/health?pretty"
pkill -f "port-forward"
Task 3: Install and Configure Fluentd
Subtask 3.1: Create Fluentd ConfigMap
Create the Fluentd configuration:

cat > fluentd-configmap.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: logging
data:
  fluent.conf: |
    <source>
      @type tail
      @id in_tail_container_logs
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_minion
      path /var/log/salt/minion
      pos_file /var/log/fluentd-salt.pos
      tag salt
      <parse>
        @type regexp
        expression /^(?<time>[^ ]* [^ ,]*)[^\[]*\[[^\]]*\]\[(?<severity>[^ \]]*) *\] (?<message>.*)$/
        time_format %Y-%m-%d %H:%M:%S
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_startupscript
      path /var/log/startupscript.log
      pos_file /var/log/fluentd-startupscript.log.pos
      tag startupscript
      <parse>
        @type syslog
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_docker
      path /var/log/docker.log
      pos_file /var/log/fluentd-docker.log.pos
      tag docker
      <parse>
        @type regexp
        expression /^time="(?<time>[^)]*)" level=(?<severity>[^ ]*) msg="(?<message>[^"]*)"( err="(?<error>[^"]*)")?( statusCode=($<status_code>\d+))?/
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_etcd
      path /var/log/etcd.log
      pos_file /var/log/fluentd-etcd.log.pos
      tag etcd
      <parse>
        @type none
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_kubelet
      multiline_flush_interval 5s
      path /var/log/kubelet.log
      pos_file /var/log/fluentd-kubelet.log.pos
      tag kubelet
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_kube_proxy
      multiline_flush_interval 5s
      path /var/log/kube-proxy.log
      pos_file /var/log/fluentd-kube-proxy.log.pos
      tag kube-proxy
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_kube_apiserver
      multiline_flush_interval 5s
      path /var/log/kube-apiserver.log
      pos_file /var/log/fluentd-kube-apiserver.log.pos
      tag kube-apiserver
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_kube_controller_manager
      multiline_flush_interval 5s
      path /var/log/kube-controller-manager.log
      pos_file /var/log/fluentd-kube-controller-manager.log.pos
      tag kube-controller-manager
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_kube_scheduler
      multiline_flush_interval 5s
      path /var/log/kube-scheduler.log
      pos_file /var/log/fluentd-kube-scheduler.log.pos
      tag kube-scheduler
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_rescheduler
      multiline_flush_interval 5s
      path /var/log/rescheduler.log
      pos_file /var/log/fluentd-rescheduler.log.pos
      tag rescheduler
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_glbc
      multiline_flush_interval 5s
      path /var/log/glbc.log
      pos_file /var/log/fluentd-glbc.log.pos
      tag glbc
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_cluster_autoscaler
      multiline_flush_interval 5s
      path /var/log/cluster-autoscaler.log
      pos_file /var/log/fluentd-cluster-autoscaler.log.pos
      tag cluster-autoscaler
      <parse>
        @type kubernetes
      </parse>
    </source>

    <filter kubernetes.**>
      @type kubernetes_metadata
      @id filter_kube_metadata
      kubernetes_url "#{ENV['FLUENT_FILTER_KUBERNETES_URL'] || 'https://' + ENV.fetch('KUBERNETES_SERVICE_HOST') + ':' + ENV.fetch('KUBERNETES_SERVICE_PORT') + '/api'}"
      verify_ssl "#{ENV['KUBERNETES_VERIFY_SSL'] || true}"
      ca_file "#{ENV['KUBERNETES_CA_FILE']}"
      skip_labels "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_LABELS'] || 'false'}"
      skip_container_metadata "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_CONTAINER_METADATA'] || 'false'}"
      skip_master_url "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_MASTER_URL'] || 'false'}"
      skip_namespace_metadata "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_NAMESPACE_METADATA'] || 'false'}"
    </filter>

    <match **>
      @type elasticsearch
      @id out_es
      @log_level info
      include_tag_key true
      host "#{ENV['FLUENT_ELASTICSEARCH_HOST'] || 'elasticsearch'}"
      port "#{ENV['FLUENT_ELASTICSEARCH_PORT'] || '9200'}"
      path "#{ENV['FLUENT_ELASTICSEARCH_PATH'] || ''}"
      scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
      ssl_verify "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERIFY'] || 'true'}"
      ssl_version "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERSION'] || 'TLSv1_2'}"
      reload_connections "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_CONNECTIONS'] || 'false'}"
      reconnect_on_error "#{ENV['FLUENT_ELASTICSEARCH_RECONNECT_ON_ERROR'] || 'true'}"
      reload_on_failure "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_ON_FAILURE'] || 'true'}"
      log_es_400_reason "#{ENV['FLUENT_ELASTICSEARCH_LOG_ES_400_REASON'] || 'false'}"
      logstash_prefix "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX'] || 'logstash'}"
      logstash_format "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_FORMAT'] || 'true'}"
      index_name "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_INDEX_NAME'] || 'logstash'}"
      type_name "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_TYPE_NAME'] || 'fluentd'}"
      <buffer>
        flush_thread_count "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_THREAD_COUNT'] || '8'}"
        flush_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_INTERVAL'] || '5s'}"
        chunk_limit_size "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE'] || '2M'}"
        queue_limit_length "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_QUEUE_LIMIT_LENGTH'] || '32'}"
        retry_max_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_RETRY_MAX_INTERVAL'] || '30'}"
        retry_forever true
      </buffer>
    </match>
EOF

# Apply the ConfigMap
kubectl apply -f fluentd-configmap.yaml
Subtask 3.2: Create Fluentd ServiceAccount and RBAC
cat > fluentd-rbac.yaml << 'EOF'
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: logging
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluentd
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - namespaces
  verbs:
  - get
  - list
  - watch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fluentd
roleRef:
  kind: ClusterRole
  name: fluentd
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: fluentd
  namespace: logging
EOF

# Apply RBAC configuration
kubectl apply -f fluentd-rbac.yaml
Subtask 3.3: Deploy Fluentd DaemonSet
cat > fluentd-daemonset.yaml << 'EOF'
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: logging
  labels:
    k8s-app: fluentd-logging
    version: v1
spec:
  selector:
    matchLabels:
      k8s-app: fluentd-logging
      version: v1
  template:
    metadata:
      labels:
        k8s-app: fluentd-logging
        version: v1
    spec:
      serviceAccount: fluentd
      serviceAccountName: fluentd
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1.14.6-debian-elasticsearch7-1.0
        env:
          - name: FLUENT_ELASTICSEARCH_HOST
            value: "elasticsearch.logging.svc.cluster.local"
          - name: FLUENT_ELASTICSEARCH_PORT
            value: "9200"
          - name: FLUENT_ELASTICSEARCH_SCHEME
            value: "http"
          - name: FLUENTD_SYSTEMD_CONF
            value: disable
          - name: FLUENT_CONTAINER_TAIL_EXCLUDE_PATH
            value: /var/log/containers/fluent*
          - name: FLUENT_ELASTICSEARCH_SSL_VERIFY
            value: "false"
          - name: FLUENT_ELASTICSEARCH_SSL_VERSION
            value: "TLSv1_2"
        resources:
          limits:
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: fluentd-config
          mountPath: /fluentd/etc
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: fluentd-config
        configMap:
          name: fluentd-config
EOF

# Deploy Fluentd DaemonSet
kubectl apply -f fluentd-daemonset.yaml
Subtask 3.4: Verify Fluentd Deployment
# Check Fluentd pods
kubectl get pods -n logging -l k8s-app=fluentd-logging

# Wait for Fluentd pods to be ready
kubectl wait --for=condition=Ready pods -l k8s-app=fluentd-logging -n logging --timeout=300s

# Check Fluentd logs
kubectl logs -n logging -l k8s-app=fluentd-logging --tail=50
Task 4: Test Logging Pipeline
Subtask 4.1: Deploy Test Applications
Create test applications that generate logs:

cat > test-app.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: log-generator
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: log-generator
  template:
    metadata:
      labels:
        app: log-generator
    spec:
      containers:
      - name: log-generator
        image: busybox
        command: ["/bin/sh"]
        args: ["-c", "while true; do echo 'Log message from log-generator pod:' $(hostname) 'at' $(date); sleep 10; done"]
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-app
  template:
    metadata:
      labels:
        app: nginx-app
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  namespace: default
spec:
  selector:
    app: nginx-app
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080
  type: NodePort
EOF

# Deploy test applications
kubectl apply -f test-app.yaml
Subtask 4.2: Generate Traffic and Logs
# Wait for test pods to be ready
kubectl wait --for=condition=Ready pods -l app=log-generator --timeout=300s
kubectl wait --for=condition=Ready pods -l app=nginx-app --timeout=300s

# Check test application logs
kubectl logs -l app=log-generator --tail=10

# Generate some nginx traffic
curl -s http://localhost:30080 > /dev/null
curl -s http://localhost:30080/nonexistent > /dev/null

# Check nginx logs
kubectl logs -l app=nginx-app --tail=10
Subtask 4.3: Verify Logs in Elasticsearch
# Port forward to Elasticsearch
kubectl port-forward -n logging svc/elasticsearch 9200:9200 &
sleep 5

# Check if indices are created
curl -X GET "localhost:9200/_cat/indices?v"

# Search for logs
curl -X GET "localhost:9200/logstash-*/_search?pretty&size=5" -H 'Content-Type: application/json' -d'
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "@timestamp": {
        "order": "desc"
      }
    }
  ]
}'

# Search for specific application logs
curl -X GET "localhost:9200/logstash-*/_search?pretty&size=5" -H 'Content-Type: application/json' -d'
{
  "query": {
    "match": {
      "kubernetes.labels.app": "log-generator"
    }
  },
  "sort": [
    {
      "@timestamp": {
        "order": "desc"
      }
    }
  ]
}'

# Stop port forwarding
pkill -f "port-forward"
Subtask 4.4: Access Kibana Dashboard
# Get node IP
NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
echo "Kibana URL: http://$NODE_IP:30601"

# Port forward to Kibana for local access
kubectl port-forward -n logging svc/kibana 5601:5601 &
echo "Kibana local URL: http://localhost:5601"

# Wait for Kibana to be ready
sleep 30

# Check Kibana health
curl -s http://localhost:5601/api/status | grep -o '"overall":{"level":"[^"]*"' || echo "Kibana is starting up..."
Subtask 4.5: Configure Kibana Index Pattern
# Create index pattern via API
curl -X POST "localhost:5601/api/saved_objects/index-pattern/logstash-*" \
  -H "Content-Type: application/json" \
  -H "kbn-xsrf: true" \
  -d '{
    "attributes": {
      "title": "logstash-*",
      "timeFieldName": "@timestamp"
    }
  }'

echo "Index pattern created. You can now access Kibana at http://localhost:5601"
echo "Go to Discover tab to view your logs"
Task 5: Advanced Configuration and Monitoring
Subtask 5.1: Monitor Fluentd Performance
# Check Fluentd resource usage
kubectl top pods -n logging -l k8s-app=fluentd-logging

# Check Fluentd detailed logs
kubectl logs -n logging -l k8s-app=fluentd-logging --tail=100 | grep -E "(error|warn|info)"

# Check Elasticsearch cluster health
kubectl port-forward -n logging svc/elasticsearch 9200:9200 &
sleep 5
curl -X GET "localhost:9200/_cluster/health?pretty"
curl -X GET "localhost:9200/_nodes/stats?pretty" | grep -A 10 "indices"
pkill -f "port-forward"
Subtask 5.2: Create Custom Log Filters
Create a custom Fluentd configuration with additional filters:

cat > fluentd-custom-config.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-custom-config
  namespace: logging
data:
  fluent.conf: |
    <source>
      @type tail
      @id in_tail_container_logs
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>

    <filter kubernetes.**>
      @type kubernetes_metadata
      @id filter_kube_metadata
    </filter>

    # Filter to exclude fluentd logs
    <filter kubernetes.**>
      @type grep
      <exclude>
        key $.kubernetes.container_name
        pattern ^fluentd$
      </exclude>
    </filter>

    # Add custom fields
    <filter kubernetes.**>
      @type record_transformer
      <record>
        cluster_name "my-k8s-cluster"
        environment "development"
      </record>
    </filter>

    # Parse nginx access logs
    <filter kubernetes.**>
      @type parser
      key_name log
      reserve_data true
      <parse>
        @type nginx
      </parse>
    </filter>

    <match **>
      @type elasticsearch
      @id out_es
      @log_level info
      include_tag_key true
      host "elasticsearch.logging.svc.cluster.local"
      port 9200
      logstash_format true
      logstash_prefix "kubernetes"
      <buffer>
        flush_interval 5s
        chunk_limit_size 2M
        queue_limit_length 32
        retry_forever true
      </buffer>
    </match>
EOF

# Apply custom configuration
kubectl apply -f fluentd-custom-config.yaml

# Update DaemonSet to use custom config
kubectl patch daemonset fluentd -n logging -p '{"spec":{"template":{"spec":{"volumes":[{"name":"fluentd-config","configMap":{"name":"fluentd-custom-config"}},{"name":"varlog","hostPath":{"path":"/var/log"}},{"name":"varlibdockercontainers","hostPath":{"path":"/var/lib/docker/containers"}}]}}}}'

# Restart Fluentd pods
kubectl rollout restart daemonset/fluentd -n logging
kubectl rollout status daemonset/fluentd -n logging
Subtask 5.3: Set Up Log Retention Policy
# Create index lifecycle policy
kubectl port-forward -n logging svc/elasticsearch 9200:9200 &
sleep 5

curl -X PUT "localhost:9200/_ilm/policy/kubernetes-logs-policy" \
  -H "Content-Type: application/json" \
  -d '{
    "policy": {
      "phases": {
        "hot": {
          "actions": {
            "rollover": {
              "max_size": "1GB",
              "max_age": "1d"
            }
          }
        },
        "delete": {
          "min_age": "7d",
          "actions": {
            "delete": {}
          }
        }
      }
    }
  }'

# Create index template with lifecycle policy
curl -X PUT "localhost:9200/_index_template/kubernetes-logs-template" \
  -H "Content-Type: application/json" \
  -d '{
    "index_patterns": ["kubernetes-*"],
    "template": {
      "settings": {
        "index.lifecycle.name": "kubernetes-logs-policy",
        "index.lifecycle.rollover_alias": "kubernetes-logs"
      }
    }
  }'

pkill -f "port-forward"
Task 6: Troubleshooting and Validation
Subtask 6.1: Common Troubleshooting Steps
# Check all logging components status
kubectl get all -n logging

# Check Fluentd configuration
kubectl get configmap fluentd-custom-config -n logging -o yaml

# Check Fluentd logs for errors
kubectl logs -n logging -l k8s-app=fluentd-logging | grep -i error

# Check Elasticsearch logs
kubectl logs -n logging -l app=elasticsearch | tail -50

# Check Kibana logs
kubectl logs -n logging -l app=kibana | tail -50

# Verify network connectivity
kubectl exec -n logging -it $(kubectl get pods -n logging -l k8s-app=fluentd-logging -o jsonpath='{.items[0].metadata.name}') -- nslookup elasticsearch.logging.svc.cluster.local

# Test Elasticsearch connectivity from Fluentd pod
kubectl exec -n logging -it $(kubectl get pods -n logging -l k8s-app=fluentd-logging -o jsonpath='{.items[0].metadata.name}') -- curl -s http://elasticsearch.logging.svc.cluster.local:9200/_cluster/health
Subtask 6.2: Performance Validation
# Check log ingestion rate
kubectl port-forward -n logging svc/elasticsearch 9200:9200 &
sleep 5

# Get indexing statistics
curl -X GET "localhost:9200/_stats/indexing?pretty"

# Check document count
curl -X GET "localhost:9200/kubernetes-*/_count?pretty"

# Monitor real-time indexing
watch -n 5 'curl -s "localhost:9200/kubernetes-*/_count" | grep count'

pkill -f "port-forward"
Subtask 6.3: Final Validation
# Generate test logs
kubectl run test-logger --image=busybox --restart=Never -- /bin/sh -c "for i in {1..20}; do echo 'Test log message number $i'; sleep 1; done"

# Wait for completion
kubectl wait --for=condition